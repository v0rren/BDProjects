lines = spark.sparkContext.textFile(input_filepath).cache()
filtered_lines = lines.filter(word: ! word.startswith("Id") & ! word.endswith("Text")) \cf3 //remove header\cf2 \
more_than_4_product_score = filtered_lines.filter(line: re.split(regex, line)[6] >= 4)
product_userID = more_than_4_product_score. \map(line: (re.split(regex, line)[1], re.split(regex, line)[2]))
userID_product = more_than_4_product_score. \map(line: (re.split(regex, line)[2], re.split(regex, line)[1]))
product_userID_reduced = product_userID.reduceByKey(a, b: a + " " + b)
userID_product_reduced = userID_product.reduceByKey(a, b: a + " " + b)
filtered_product_userID_reduced = product_userID_reduced.filter(x: len(x[1].split(" ")) > 1). \ map(x: (x[0], set(x[1].split(" "))))
filtered_userID_product_reduced = userID_product_reduced.map(x: (x[0], set(x[1].split(" "))))
product_userID_dictionary = filtered_product_userID_reduced.collectAsMap()
userID_product_dictionary = filtered_userID_product_reduced.collectAsMap()
final_dict = {}
for product in product_2_userID_dictionary.keys(){
	pairs = list(itertools.combinations(product_2_userID_dictionary[product], 2))
		for pair in pairs{
        if pair & tuple(reversed(pair)) ! in final_dict{
            products_intersection = userID_product_dictionary[pair[0]]. \
                intersection(userID_product_dictionary[pair[1]])}
            if len(products_intersection) >= 3{
                final_dict[pair] = products_intersection}
     }
 }
final = sorted(final_dict.items(), x: (x[0], x[1]))
collapsed = final.coalesce(1)
saveAsTextFile(collapsed_)
